---
title: "Anglemania on parallelized steroids"
output:
  html_document:
    df_print: paged
---

```{r}
library(data.table)
library(dplyr)
library(Seurat)
library(SeuratDisk)
library(bigstatsr)
# library(igraph)
# library(BPCells)
```


```{r}
# devtools::unload("anglemania")
devtools::load_all("../anglemania/")
```

```{r}
sessioninfo <- sessionInfo()
# sessioninfo['otherPkgs']
```

# load data
```{r}
load("/data/bimsbstatic/public/akalin/akollot/Neuroblastoma/Results/Zethoven_Tothill_natcomm_2022_PCPG/anglemania/Zethoven_example_dataset.RDS")

```

# run anglemana
## anglemanise
```{r}
sl <- sl_raw[1:10]
names(sl) <- gsub("_summarised_seacell.h5seurat", "", basename(names(sl)))
sl[[1]]

# Get the list of count matrices
system.time({ # ~ 2 mins 18 secs for 10 samples with parallelization on 10 cores
  l_processed <- big_anglemanise(sl, extrema = 0.005, n_cores = 10)
  # I'm using pblapply (which is like mclapply from the parallel package but with a progress bar).
  # When the number of cores is equal to the number of samples, the progress bar will not
  # be very helpful cause it'll jump from 0 to 100%.
})

names(l_processed) <- names(sl)


integration_features <- big_extract_integration_features(l_processed, cutoff = 7)

integration_order <- get_integration_order(l_processed)
# using the approach that Artem implemented, just the way to get there is different.
# Basically you count how many correlated gene pairs are shared between two samples.
# Currently, I don't subset the gene pairs to contain only the genes from "integration_features". 
# Have to Explore if that changes the behaviour, or introduces a bias ==> samples with a 
# lot of genes might be favored to be integrated first just because they have a higher probability to share genes.
# Could also "normalize" the counts based on the number of genes per sample?
```

```{r}
seurat_combined <- integrate_by_features(sl,
  features_to_integrate = integration_features,
  int_order = integration_order
)
```



# explore gaussian fit and angle distribution
```{r}
x_mat <- LayerData(sl_raw[[1]], layer = "counts") %>% as.matrix()

x_mat_ang <- big_extract_angles(x_mat)

l_angles <- big_approximate_angles(x_mat_ang, quantile_split = 0.005)

l_angles <- fit_gaussian_cor(l_angles)

l_angles$statistics$mu
l_angles$statistics$sigma
l_angles$angles_dist$prob_gauss
l_angles$angles_dist$angle

ref_angles <- l_angles$angles_dist$angle
gauss_quants <- l_angles$angles_dist$prob_gauss
##
extreme <- 0.03
extrema <- c(extreme, 1-extreme)
crit_vals <- purrr::map_dbl(
  extrema,
  ~ ref_angles[which.min(abs(gauss_quants - .x))]
)
crit_vals

results_list <- big_factorise(x_mat, name = "test", extrema = extrema)
str(results_list)


```



## check out pblapply for progressbar
```{r}
list_x_mats <- parallel::mclapply(sl, function(x) {
  x <- SeuratObject::GetAssayData(x, assay = "RNA")
}, mc.cores = n_cores)

pboptions(type = "timer", style = 1, char = "=")
test <- pbapply::pblapply(sl, function(x){
  x <- SeuratObject::GetAssayData(x, assay = "RNA")
  Sys.sleep(3)
}, cl = 5)
```


### check sampling 
```{r}
X <- SeuratObject::GetAssayData(sl[[1]], assay = "RNA") %>%
  as.matrix()[1:1000,] %>%
  as_FBM()

# log normalize the data
big_apply(X, a.FUN = function(X, ind) {
  X.sub <- X[, ind, drop = FALSE]
  # normalize the data:
  #   divide gene counts by the number of total counts per cell,
  #   multiply by 10,000 (scaling factor like in Seurat)
  X.sub <- t(t(X.sub) / colSums(X.sub) * 10000)
  X.sub <- log1p(X.sub)

  X[, ind] <- X.sub
  NULL
})

X_c <- big_copy(X)
# X_g <- big_copy(X) %>% big_transpose()

# Calculate correlation matrix
# first transpose the matrix because big_cor calculates the covariance (XT*X)
X <- big_transpose(X)

X <- big_cor(X, block.size = 1000)


# apply sampling on the original FBM (X) and store the result in perm_mat
# cols are the cells currently
# shuffle cells
big_apply(X_c, a.FUN = function(X, ind) {
  print(min(ind))
  X_c[, ind] <- apply(X[, ind], 2, sample)
  NULL
}, a.combine = "c", ncores = 1)

X_c <- big_transpose(X_c)
X_c <- big_cor(X_c, block.size = 1000)

X[1:5, 1:5]
X_c[1:5, 1:5]

test <- density(X[])

plot(density(X_c[]))
# shuffle genes
# does that even make sense??
# ==> I'm gonna compare every pair of genes anyways, 
#   hence changing the order of genes (shuffling) will not alter the results
# big_apply(X_g, a.FUN = function(X, ind) {
#   print(min(ind))
#   X_g[, ind] <- apply(X[, ind], 2, sample)
#   NULL
# }, a.combine = "c", ncores = 1)

# X_g <- big_cor(X_g, block.size = 1000)

```